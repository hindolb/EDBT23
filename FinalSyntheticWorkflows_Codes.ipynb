{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Mounting Google Drive- The output files are saved in ./drive/MyDrive/EDBT23/files directory"
      ],
      "metadata": {
        "id": "wKwF4PNxRomw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waThcOarkpXU",
        "outputId": "fcb6e27c-e4ec-44ed-bb59-6ac540aee763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Removing all previous results- Run this when working on a new dataset or completely re-running the existing dataset"
      ],
      "metadata": {
        "id": "2FY7N94uSjZR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s4XGzFMMVga7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f622a688-dc36-4e92-a056-e6c40813f957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './drive/MyDrive/EDBT23/files/results/*.pkl': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm ./drive/MyDrive/EDBT23/files/results/*.pkl\n",
        "! rm -rf ./drive/MyDrive/EDBT23/files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tIZLltiqptm"
      },
      "source": [
        "3a. Importing dataset (in this case CYERSHAKE) from mounted google drive. Change name from CYBERSHAKE to GENOME/MONTAGE as applicable. Not necessary for WEBLOG. For WEBLOG follow from 3b after 4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjDZxXy3plMf"
      },
      "outputs": [],
      "source": [
        "!cp ./drive/MyDrive/EDBT23/SyntheticWorkflows.tar.gz ./\n",
        "!tar -xf ./SyntheticWorkflows.tar.gz\n",
        "!rm ./SyntheticWorkflows.tar.gz\n",
        "!mv ./SyntheticWorkflows/MONTAGE/ ./dataset/\n",
        "!rm -rf SyntheticWorkflows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwZSVMTYq9m9"
      },
      "source": [
        "4. Extracting information from imported XML files in the imported dataset above. Seperating training and test set. Creating and saving list of files for tasks. Creating tasks for files (inverse of files for tasks). Saving task runtime and file size.\n",
        "NOTE: Matrix operation are not used in this work. Not necessary for WEBLOG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkFTCG5Eq00y",
        "outputId": "7b3087ab-46d1-4b66-e85d-3800fbe09217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting from XML\n",
            "Starting train-test split\n",
            "Finished splitting data\n",
            "Finished XML   extraction\n",
            "Number of training tasks:  100060\n",
            "Number of testing tasks:  11100\n",
            "Saving extracted data\n",
            "Finished saving extracted data\n",
            "Creating task ID and file ID\n",
            "Initttializing matrix\n",
            "Dimension:  (100060, 872)\n",
            "Creating matrix and file ID\n",
            "Done creatiiing matrix, Saving Data\n",
            "Done saving data\n"
          ]
        }
      ],
      "source": [
        "import requests \n",
        "import xml.etree.ElementTree as ET \n",
        "import os\n",
        "import joblib as jl\n",
        "import numpy as np\n",
        "from scipy import linalg \n",
        "from scipy.spatial import distance\n",
        "from scipy.sparse import lil_matrix\n",
        "\n",
        "\n",
        "def matrixOperation(train_tasks):\n",
        "\tfiles = []\n",
        "\ttasks = []\n",
        "\tnum_files = []\n",
        "\tnum_tasks = []\n",
        "\tfile_id_mapping = {}\n",
        "\tid_file_mapping = {}\n",
        "\ttask_id_mapping = {}\n",
        "\tid_task_mapping = {}\t\n",
        "\ttemp = []\n",
        "\tcount1 = 0\n",
        "\tcount2 = 0\n",
        "\tprint (\"Creating task ID and file ID\")\n",
        "\tfor tasks in train_tasks.keys():\n",
        "\t\ttask_id_mapping[tasks] = count1\n",
        "\t\tid_task_mapping[count1] = tasks\n",
        "\t\tcount1 += 1\n",
        "\t\tfor file1 in train_tasks[tasks]:\n",
        "\t\t\tif file1 not in file_id_mapping.keys():\n",
        "\t\t\t\tfile_id_mapping[file1] = count2\n",
        "\t\t\t\tid_file_mapping[count2] = file1\n",
        "\t\t\t\tcount2 += 1\n",
        "\tnum_files = count2\n",
        "\tdel temp\n",
        "\tnum_tasks = count1\n",
        "\tprint (\"Initttializing matrix\")\n",
        "\tfile_task_matrix = lil_matrix((num_tasks,num_files), dtype=np.int8).toarray()\n",
        "\tprint (\"Dimension: \", file_task_matrix.shape)\n",
        "\tcount1 = 0\n",
        "\tprint (\"Creating matrix and file ID\")\n",
        "\tfor tasks in train_tasks.keys():\n",
        "\t\tfor file1 in train_tasks[tasks]:\n",
        "\t\t\ttask_id = task_id_mapping[tasks]\n",
        "\t\t\tfile_id = file_id_mapping[file1]\n",
        "\t\t\ttemp =file_task_matrix[task_id][file_id]\n",
        "\t\t\ttemp += 1\n",
        "\t\t\tfile_task_matrix[task_id][file_id] = temp \n",
        "\tfile_task_matrix = np.transpose(file_task_matrix)\t \t\t\n",
        "\tprint (\"Done creatiiing matrix, Saving Data\")\n",
        "\twith open(\"File_Task_Matrix.pkl\", 'wb') as fp:\n",
        "\t\tjl.dump(file_task_matrix, fp)\n",
        "\tfp.close()\n",
        "\tdel file_task_matrix\n",
        "\twith open(\"Id_File_Mapping.pkl\", 'wb') as fp:\n",
        "\t\tjl.dump(id_file_mapping, fp)\n",
        "\tfp.close()\n",
        "\twith open(\"Id_Task_Mapping.pkl\", 'wb') as fp:\n",
        "\t\tjl.dump(id_task_mapping, fp)\n",
        "\tfp.close()\n",
        "\tdel id_file_mapping\n",
        "\tdel id_task_mapping\t\n",
        "\twith open(\"File_Id_Mapping.pkl\", 'wb') as fp:\n",
        "\t\tjl.dump(file_id_mapping, fp)\n",
        "\tfp.close()\n",
        "\twith open(\"Task_Id_Mapping.pkl\", 'wb') as fp:\n",
        "\t\tjl.dump(task_id_mapping, fp)\n",
        "\tfp.close()\n",
        "\tprint (\"Done saving data\")\n",
        "\t\n",
        "\t\t\t\n",
        "def test_train_split(data, task_runtime):\n",
        "\tprint (\"Starting train-test split\")\n",
        "\ttrain_task = {}\n",
        "\ttest_task = {}\n",
        "\tcount = 0\n",
        "\tdata_file = {}\n",
        "\tfile_runtime = {}\n",
        "\tmarker = 0\n",
        "\tfor task1 in data.keys():\n",
        "\t\tfor task2 in data:\t\t\n",
        "\t\t\tif len(set(data[task1]).intersection(set(data[task2]))) != 0:\n",
        "\t\t\t\tif count >= (0.2*len(data.values())):\n",
        "\t\t\t\t\tmarker = 1\t\t\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\ttest_task[task1] = data[task1]\n",
        "\t\t\t\t\ttrain_task[task2] = data[task2]\n",
        "\t\t\t\t\tcount += 2\t\t\t\t\n",
        "\t\t\t\t\tbreak\n",
        "\t\tif marker == 1:\n",
        "\t\t\tbreak\n",
        "\tfor task1 in data.keys():\n",
        "\t\tif task1 not in test_task.keys(): \n",
        "\t\t\tif task1 not in train_task.keys():\n",
        "\t\t\t\ttrain_task[task1] = data[task1]\n",
        "\tprint (\"Finished splitting data\")\t\t\n",
        "\tfor tasks in train_task.keys():\n",
        "\t\tfile_colls = train_task[tasks]\n",
        "\t\truntime = task_runtime[tasks]\n",
        "\t\tfor files in file_colls:\n",
        "\t\t\ttry:\n",
        "\t\t\t\ttemp = data_file[files]\n",
        "\t\t\t\ttemp.append(tasks)\n",
        "\t\t\t\tdata_file[files] = temp\n",
        "\t\t\t\ttemp = file_runtime[files]\n",
        "\t\t\t\tfile_runtime[files] = temp + runtime\n",
        "\t\t\texcept:\n",
        "\t\t\t\ttemp = [tasks]\n",
        "\t\t\t\tdata_file[files] = temp\n",
        "\t\t\t\tfile_runtime[files] = 0\n",
        "\treturn train_task, test_task, data_file, file_runtime\n",
        "\n",
        "\n",
        "def parseXML(xmlfile, data, file_size, task_runtime): \n",
        "\ttry:\n",
        "\t\t# create element tree object \n",
        "\t\ttree = ET.parse(xmlfile) \n",
        "  \t\n",
        "\t\t# get root element \n",
        "\t\troot = tree.getroot() \n",
        "\t\n",
        "\t\tfor child in root:\n",
        "\t\t\ttry:\n",
        "\t\t\t\t#Check if task execution time and file size are in int/float\n",
        "\t\t\t\ttask_id = xmlfile+\"_\"+child.attrib['id']\n",
        "\t\t\t\ttask_runtime[task_id] = float(child.attrib['runtime'])\n",
        "\t\t\t\tdata[task_id] = list()\n",
        "\t\t\t\tfor grn_child in child:\n",
        "\t\t\t\t\tif grn_child.attrib['link'] == \"input\":\t\n",
        "\t\t\t\t\t\tdata[task_id].append(grn_child.attrib['file'])\n",
        "\t\t\t\t\t\tfile_size[grn_child.attrib['file']] = grn_child.attrib['size']\n",
        "\t\t\texcept:\n",
        "\t\t\t\tcontinue\n",
        "\texcept:\n",
        "\t\tprint (\"This XML file could not be processed\")\t\n",
        "def main(): \n",
        "\n",
        "\t# directory to store the file dependencises of each task\n",
        "\t# key: task_id, value = file_id \n",
        "\tdata = {}\n",
        "\tfile_size = {}\n",
        "\ttask_runtime = {}\n",
        "\n",
        "\t# iterarating over all files in the directory\n",
        "\tprint (\"Extracting from XML\")\n",
        "\tdirs = \"./dataset\"\n",
        "\tfor files in os.listdir(os.path.join(\"./\",dirs)):\n",
        "\t\tif files[-3:] == \"dax\":\n",
        "\t\t\t# parse xml file \n",
        "\t\t\tparseXML(os.path.join(os.path.join(\"./\",dirs),files), data, file_size, task_runtime) \n",
        "\n",
        "\ttrain_task, test_task, data_file, file_runtime = test_train_split(data, task_runtime)\n",
        "\tprint (\"Finished XML   extraction\")\n",
        "\tprint(\"Number of training tasks: \", len(train_task))\n",
        "\tprint(\"Number of testing tasks: \", len(test_task))\n",
        "\n",
        "\t# saving extracted data\n",
        "\n",
        "\tprint(\"Saving extracted data\")\n",
        "\tnew_file = open(\"TrainingTasks.pkl\", \"wb\")\n",
        "\tjl.dump(train_task,new_file)\n",
        "\tnew_file.close()\n",
        "\tnew_file = open(\"TestingTasks.pkl\", \"wb\")\n",
        "\tjl.dump(test_task,new_file)\n",
        "\tnew_file.close()\n",
        "\tnew_file = open(\"TrainingFiles.pkl\", \"wb\")\n",
        "\tjl.dump(data_file,new_file)\n",
        "\tnew_file.close()\n",
        "\tnew_file = open(\"TrainingFileRuntime.pkl\", \"wb\")\n",
        "\tjl.dump(file_runtime,new_file)\n",
        "\tnew_file.close()\t\n",
        "\tnew_file = open(\"FileSize.pkl\", \"wb\")\n",
        "\tjl.dump(file_size,new_file)\n",
        "\tnew_file.close()\t\t\n",
        "\tprint(\"Finished saving extracted data\") \n",
        "\t\n",
        "\tnew_file = open(\"TrainingTasks.pkl\", \"rb\")\n",
        "\ttrain_tasks = jl.load(new_file)\n",
        "\tnew_file.close()\t\t\n",
        "\tmatrixOperation(train_tasks)\n",
        "      \n",
        "if __name__ == \"__main__\": \n",
        "  \n",
        "\t# calling main function \n",
        "\tmain() "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3b. Importing dataset- WEBLOG from mounted google drive."
      ],
      "metadata": {
        "id": "bTzPfzGyTcZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./drive/MyDrive/EDBT23/TestingTasks.pkl ./\n",
        "!cp ./drive/MyDrive/EDBT23/TrainingTasks.pkl ./\n",
        "!cp ./drive/MyDrive/EDBT23/TrainingFiles.pkl ./"
      ],
      "metadata": {
        "id": "96FVZ3GNTp1-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_RgbDl5rhof"
      },
      "source": [
        "5. Creating directory structure for storing the results of networkx graph and final results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0TNNmZo_rkN7"
      },
      "outputs": [],
      "source": [
        "!mkdir ./drive/MyDrive/EDBT23/files\n",
        "!mkdir ./drive/MyDrive/EDBT23/files/Nodes\n",
        "!mkdir ./drive/MyDrive/EDBT23/files/Nodes/Graphs\n",
        "!mkdir ./drive/MyDrive/EDBT23/files/Nodes/Weights\n",
        "!mkdir ./drive/MyDrive/EDBT23/files/Nodes/Graphs/subgraphs\n",
        "!mkdir ./drive/MyDrive/EDBT23//files/results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmsrqn3nrVY2"
      },
      "source": [
        "6. From the extracted data above, create the networkx graph and record the weight of the edges of the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxqNgsXnrUxL",
        "outputId": "90b6b505-b24d-4426-d7e1-e90003637b0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading graph\n",
            "Creating edges of the graph\n",
            "Successfully created a graph\n",
            "Saving the created graph\n",
            "Successfully computed the weights\n",
            "Saving the computed weights\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import joblib as jl\n",
        "#from memory_profiler import profile\n",
        "import os\n",
        "import re\n",
        "\n",
        "def graphCreator(files_list1, filename1):\n",
        "\n",
        "\tdirectory_graph = \"./drive/MyDrive/EDBT23/files/Nodes/Graphs\"\n",
        "\tdirectory_weight = \"./drive/MyDrive/EDBT23/files/Nodes/Weights\"\t\n",
        "\tgraph_file = filename = os.path.join(directory_graph, \"Graph.pkl\")\t\t\t\t\t\t\t\n",
        "\tgraph_file = re.sub(r'(ExtractedWorkflowData)','Graph', filename)\n",
        "\tweight_file = filename = os.path.join(directory_weight, \"Weight.pkl\")\t\t\t\t\t\t\t\n",
        "\tweight_file = re.sub(r'(ExtractedWorkflowData)','Weight', filename)\n",
        "\tg= nx.Graph()\n",
        "\tprint (\"Creating edges of the graph\")\n",
        "\tfor it1 in files_list1.keys():\n",
        "\t\tfor it2 in files_list1.keys():\n",
        "\t\t\ttask1 = files_list1[it1]\n",
        "\t\t\ttask2 = files_list1[it2]\n",
        "\t\t\tcommon = len(list(set(task1).intersection(set(task2))))\n",
        "\t\t\tif common != 0:\n",
        "\t\t\t\tg.add_edge(it1, it2, weight = common)\t\t\t\t\t\n",
        "\tprint (\"Successfully created a graph\")\t\n",
        "\tprint(\"Saving the created graph\")\n",
        "\tnew_file = open(graph_file, \"wb\")\n",
        "\tjl.dump(g, new_file)\n",
        "\tnew_file.close()\n",
        "\tdel g\n",
        "\t\n",
        "\tweight = {}\n",
        "\tfor it1 in files_list1.keys():\n",
        "\t\tfor it2 in files_list1.keys():\n",
        "\t\t\ttask1 = files_list1[it1]\n",
        "\t\t\ttask2 = files_list1[it2]\n",
        "\t\t\tcommon = len(list(set(task1).intersection(set(task2))))\n",
        "\t\t\tif common != 0:\n",
        "\t\t\t\tweight[it1+\"///...\"+it2] = common\n",
        "\tprint (\"Successfully computed the weights\")\t\n",
        "\tprint(\"Saving the computed weights\")\n",
        "\tnew_file = open(weight_file, \"wb\")\n",
        "\tjl.dump(weight, new_file)\n",
        "\tnew_file.close()\t\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\t\n",
        "\tfilename = \"./TrainingFiles.pkl\"\n",
        "\tprint (\"Loading graph\")\n",
        "\tnew_file = open(filename, \"rb\")\n",
        "\tfile_list1 = jl.load(new_file)\n",
        "\tnew_file.close()\n",
        "\n",
        "\tgraphCreator(file_list1, filename)\n",
        "\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeHwI03UsC2E"
      },
      "source": [
        "7. Finding and saving all the component subgraphs which have number of nodes greater than 1. \n",
        "Also, finding edge-to-node ratio- mean, max, min and SD.\n",
        "Saving edge-to-node ratio "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQhNjeXw3ChE",
        "outputId": "c3e7efd5-d816-4365-e9f8-707fb04b268b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading graph of node:  Graph.pkl\n",
            "11\n",
            "Saving the graph\n",
            "Saving the graph\n",
            "Saving the graph\n",
            "Saving the graph\n",
            "Saving the graph\n",
            "Saving the edge-node ratio\n",
            "Mean of edge-to-node ratio 21.388972431077697\n",
            "Standard Deviation of edge-node ratio:  37.952297392997814\n",
            "Maximum node-to-edge ratio:  97.2781954887218\n",
            "Minimum node-to-edge ratio:  1.5\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import joblib as jl\n",
        "import networkx.algorithms.components as comp\n",
        "import os\n",
        "import re, math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def find_subgraphs(filename, G): \n",
        "\tS = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
        "\tcount = 0\n",
        "\tdirectory = \"./drive/MyDrive/EDBT23/files/Nodes/Graphs/subgraphs\"\n",
        "\tratio2 = []\n",
        "\tmean2_1 = []\n",
        "\tsd2_1 = []\n",
        "\trat_list = {}\n",
        "\tfor it1 in S:\n",
        "\t\tif len(it1.nodes) > 1:\n",
        "\t\t\tcount += 1\n",
        "\t\t\tfilename1 = re.sub(r'(./drive/MyDrive/EDBT23/files/Nodes/Graphs/)',\"./drive/MyDrive/EDBT23/files/Nodes/Graphs/subgraphs/\", filename)\t\t\t\n",
        "\t\t\tfilename1 = re.sub(r'(.pkl)','_'+str(count)+'.pkl', filename1)\n",
        "#\t\t\tfilename = os.path.join(directory, filename)\t\t\t\t\t\n",
        "\t\t\tprint (\"Saving the graph\")\t\t\t\n",
        "\t\t\tnew_file = open(filename1, \"wb\")\n",
        "\t\t\tjl.dump(it1, new_file)\n",
        "\t\t\tnew_file.close()\n",
        "\t\t\tdel filename1\n",
        "\t\t\tratio2.append(len(list(it1.edges))/len(list(it1.nodes)))\n",
        "\t\t\trat_list[count] = len(list(it1.edges))/len(list(it1.nodes))\n",
        "\tmean2 = 0\n",
        "\tsd2 = 0\n",
        "\tfor it1 in ratio2:\n",
        "\t\tmean2 += it1\n",
        "\tmean2 = mean2/len(ratio2)\n",
        "\tmean2_1.append(mean2)\n",
        "\ttemp = 0\n",
        "\tfor it1 in ratio2:\n",
        "\t\ttemp += ((it1 - mean2)**2)\n",
        "\ttemp = temp/len(ratio2)\n",
        "\tsd2 = math.sqrt(temp)   \n",
        "\tsd2_1.append(sd2)\n",
        "\n",
        "\tprint (\"Saving the edge-node ratio\")\t\t\t\n",
        "\tnew_file = open(\"./drive/MyDrive/EDBT23/files/results/ratio.pkl\", \"wb\")\n",
        "\tjl.dump(rat_list, new_file)\n",
        "\tnew_file.close()\t\t\t  \n",
        " \n",
        "\tprint (\"Mean of edge-to-node ratio\", mean2)  \n",
        "\tprint(\"Standard Deviation of edge-node ratio: \", sd2)\n",
        "\tprint (\"Maximum node-to-edge ratio: \", max(ratio2))\n",
        "\tprint (\"Minimum node-to-edge ratio: \", min(ratio2)) \n",
        " \n",
        "if __name__==\"__main__\":\n",
        "\t\n",
        "\tdirectory = \"./drive/MyDrive/EDBT23/files/Nodes/Graphs\"\n",
        "\tfor filename1 in os.listdir(directory):\n",
        "\t\tif filename1[-3:] != \"pkl\":\n",
        "\t\t\tcontinue\t\t\t\t\t\n",
        "\t\tprint (\"Loading graph of node: \", filename1)\n",
        "\t\tfilename = os.path.join(directory, filename1)\t\t\t\t\t\n",
        "\t\tnew_file = open(filename, \"rb\")\n",
        "\t\tg = jl.load(new_file)\n",
        "\t\tnew_file.close()\n",
        "#\tprint (\"Loading weight of node: \", counter)\n",
        "#\tnew_file = open(\"./files/Nodes/Weights/Weight_\" +str(counter) +\"_.pkl\", \"rb\")\n",
        "#\tweight = jl.load(new_file)\n",
        "#\tnew_file.close()\n",
        "\n",
        "#\tg= nx.Graph()\n",
        "#\tfor it1 in weight.keys():\n",
        "#\t\tfiles = it1.split(\"_\")\n",
        "#\t\tg.add_edge(files[0], files[1])\n",
        "\n",
        "\t\tprint(nx.number_connected_components(g))\n",
        "#\tmaxx = max(nx.connected_component_subgraphs(g), key=len)\n",
        "#\tprint (maxx.nodes)\n",
        "#\tprint (len(weight))\n",
        "\t\tfind_subgraphs(filename, g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPMB8yfmfOyJ"
      },
      "source": [
        "8. Find betweenness centrality for all subgraphs having number of nodes greater than 1. \n",
        "Saving the betweenness centrality as a dictionary-\n",
        "Key: betweenness centrality\n",
        "Value: List of edges which have a betweenness centrality value = key\n",
        "Sorting the dictionary in descending order (of Key)\n",
        "Saving the dictionary containing sorted betweenness centrality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HcbVzMHTzt7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b840af-5ae5-42bd-e611-a718e07407e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and sorting the edges by weight\n",
            "Loading the node-edge ratio\n",
            "Time to find betweenness centrality of all nodes (non-parallel):  11.916260004043579\n",
            "Time to find betweenness centrality of all nodes (non-parallel):  0.00022602081298828125\n",
            "Time to find betweenness centrality of all nodes (non-parallel):  0.00044465065002441406\n",
            "Time to find betweenness centrality of all nodes (non-parallel):  0.0004477500915527344\n",
            "Time to find betweenness centrality of all nodes (non-parallel):  0.00014901161193847656\n"
          ]
        }
      ],
      "source": [
        "import joblib as jl\n",
        "import os\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "import itertools\n",
        "import networkx as nx\n",
        "import sys\n",
        "import re\n",
        "import collections\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib.pyplot as plt\n",
        "import traceback\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def non_parallel(g):\n",
        "\tstart = time.time()  \n",
        "\td1 = nx.edge_betweenness_centrality(g, weight=\"common\")\n",
        "\tend = time.time()\n",
        "\td2 = {}\n",
        "\tprint (\"Time to find betweenness centrality of all nodes (non-parallel): \", end-start)\n",
        "\tstart = time.time()\n",
        "\tfor it1 in d1.keys():\n",
        "\t\tval = d1[it1]\n",
        "\t\tif val in d2.keys():\n",
        "\t\t\ttemp = d2[val]\n",
        "\t\t\ttemp.append(str(it1[0]) +\"///...\" +str(it1[1]))\n",
        "\t\t\td2[val] = temp\n",
        "\t\telse:\n",
        "\t\t\td2[val] = [str(it1[0]) +\"///...\" +str(it1[1])]\n",
        "\t\t\n",
        "\treturn d2\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "\tprint (\"Loading and sorting the edges by weight\")\n",
        "\tfilename = \"./drive/MyDrive/EDBT23/files/Nodes/Weights/Weight.pkl\"\n",
        "\tnew_file = open(filename, \"rb\")\n",
        "\tweights = jl.load(new_file)\n",
        "\tnew_file.close()\n",
        "\tsorted_weights = {k: v for k, v in sorted(weights.items(), key=lambda item: item[1])}\t\t\n",
        "\tgl_rank = {}\n",
        "\tcount = 1\n",
        "\tfor edges in sorted_weights.keys():\n",
        "\t\tgl_rank[edges] = count\n",
        "\t\tcount += 1\n",
        "\t\n",
        "\tprint (\"Loading the node-edge ratio\")\t\t\t\n",
        "\tnew_file = open(\"./drive/MyDrive/EDBT23/files/results/ratio.pkl\", \"rb\")\n",
        "\tratio = jl.load(new_file)\n",
        "\tnew_file.close()\t\t\t\n",
        "\n",
        "\tdirectory = \"./drive/MyDrive/EDBT23/files/Nodes/Graphs/subgraphs\" \n",
        "\tcount = 1\n",
        "\ttau_coll_fin = {}\n",
        "\tp_coll_fin = {}\t\n",
        "\tx = []\n",
        "\ty = []\n",
        "\tp = []\n",
        "\tfor filename1 in os.listdir(directory): \n",
        "\t\tif filename1[0:5] == \"Graph\":\n",
        "\t\t\tfilename = os.path.join(directory, filename1)\t\t\t\n",
        "\t\t\tnew_file = open(filename, \"rb\")\n",
        "\t\t\tG = jl.load(new_file)\n",
        "\t\t\tnew_file.close()\t\t\t\n",
        "\t\t\ttuples = non_parallel(G)\n",
        "\t \t\t#Save the betweenness centrality values and the edges\n",
        "\t\t\tfilename = os.path.join(directory, \"Tuple_\"+str(count)+\".tup\")\t\t\t\t\t\t\t\t\n",
        "\t\t\tnew_file = open(filename, \"wb\")\n",
        "\t\t\tjl.dump(tuples, new_file)\n",
        "\t\t\tnew_file.close()\t\n",
        "\t\t\t#Save the betweenness centrality values- sorted \n",
        "\t\t\tsorted_tuples = dict(sorted(tuples.items(), reverse = True)) #Check order of sorting\n",
        "\t\t\tfilename = os.path.join(directory, \"SortedTuple_\"+str(count)+\".tup\")\t\t\t\t\t\t\t\t\n",
        "\t\t\tnew_file = open(filename, \"wb\")\n",
        "\t\t\tjl.dump(sorted_tuples, new_file)\n",
        "\t\t\tnew_file.close()\t\n",
        "\t\t\t\t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdokz_Toncbf"
      },
      "source": [
        "9. Compute threshold according to equation 1 in the paper- considering the entire graph\n",
        "Delete edges from the graph\n",
        "Find the subgraph \n",
        "For each test tasks\n",
        "  For each subgraph\n",
        "    Find TP, FP, FN and compute F-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a3KaY0-QWDNf"
      },
      "outputs": [],
      "source": [
        "from matplotlib.units import ma\n",
        "import joblib as jl\n",
        "import os, re\n",
        "import networkx as nx\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "with open(\"./TestingTasks.pkl\", 'rb') as fp:\n",
        "  test_set = jl.load(fp)\n",
        "fp.close()\n",
        "\n",
        "with open(\"./drive/MyDrive/EDBT23/files/Nodes/Weights/Weight.pkl\", 'rb') as fp:\n",
        "  weight = jl.load(fp)\n",
        "fp.close()\n",
        "\n",
        "betweenness_coll = {}\n",
        "for filename in os.listdir(\"./drive/MyDrive/EDBT23/files/Nodes/Graphs/subgraphs\"):\n",
        "  if filename[0:6] != \"Sorted\":\n",
        "    continue\n",
        "  #Open the file for the corresponding subgraph\n",
        "  filename1 = re.sub(r'(SortedTuple)',\"Graph\", filename)\t\t\t\n",
        "  filename1 = re.sub(r'(.tup)',\".pkl\", filename1)\t\t\t\n",
        "  try:\n",
        "    with open(os.path.join(\"./drive/MyDrive/EDBT23/files/Nodes/Graphs/subgraphs\", filename1), 'rb') as fp:\n",
        "      G = jl.load(fp)\n",
        "    fp.close()\n",
        "    G_nodes = G.nodes()\n",
        "    G_edges = G.edges()\n",
        "  except:\n",
        "    print(filename1)\n",
        "    continue\n",
        "  with open(os.path.join(\"./drive/MyDrive/EDBT23/files/Nodes/Graphs/subgraphs\", filename), 'rb') as fp:\n",
        "    betweenness = jl.load(fp)\n",
        "  fp.close()\n",
        "  betweenness_coll[list(betweenness.keys())[0]] = betweenness[list(betweenness.keys())[0]]\n",
        "  del betweenness\n",
        "\n",
        "sum_of_wts = 0\n",
        "sorted_betweeness_coll = dict(sorted(betweenness_coll.items(), reverse = True)) #Check order of sorting\n",
        "selected_edges = list(sorted_betweeness_coll)[0]\n",
        "selected_edges = []\n",
        "for it1 in sorted_betweeness_coll.keys():\n",
        "  edges = sorted_betweeness_coll[it1]\n",
        "  for it2 in edges:\n",
        "    selected_edges.append(it2)\n",
        "count = 1\n",
        "for it1 in selected_edges:\n",
        "  sum_of_wts += weight[it1]\n",
        "  count += 1\n",
        "thr = sum_of_wts/count\n",
        "\n",
        "new_file = open(\"./drive/MyDrive/EDBT23/files/Nodes/Graphs/Graph.pkl\", \"rb\")\n",
        "G = jl.load(new_file)\n",
        "new_file.close()\n",
        "\n",
        "for edges in selected_edges:\n",
        "  if weight[edges] < thr:\n",
        "    edge = edges.split(\"///...\")\n",
        "    G.remove_edge(edge[0], edge[1])\n",
        "S = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
        "\n",
        "all_nodes = G.nodes()\n",
        "node_coll = {}\n",
        "count = 1\n",
        "for subgraphs in S:\n",
        "  if len(subgraphs.nodes) > 1:\n",
        "    node_coll[count] = subgraphs.nodes\n",
        "    count +=1\n",
        "\n",
        "tp_f = {}\n",
        "tn_f = {}\n",
        "fp_f = {}\n",
        "fn_f = {}\n",
        "f1_f = {}\n",
        "maxx_f1_f = {}\n",
        "maxx_f = {}\n",
        "map = {}\n",
        "for tasks in test_set:\n",
        "  test_files = test_set[tasks]\n",
        "  tp = {}\n",
        "  tn = {}\n",
        "  fp = {}\n",
        "  fn = {}\n",
        "  f1 = {} \n",
        "  max_f1 = 0 \n",
        "  maxx = 0\n",
        "  count = 1\n",
        "  for it1 in node_coll.keys():\n",
        "    files = node_coll[it1]\n",
        "    tp[count] = len(set(test_files).intersection(set(files)))/len(set(test_files))\n",
        "    notin_test = set(all_nodes).difference(set(test_files))\n",
        "    notin_pred = set(all_nodes).difference(set(files))\n",
        "    tn[count] = len(notin_test.intersection(notin_pred))\n",
        "    fp[count] = len(set(files).difference(set(test_files)))/len(set(all_nodes))\n",
        "    fn[count] = len(set(test_files).difference(set(files)))/len(set(all_nodes))\n",
        "    f1[count] = (2*tp[count])/((2*tp[count])+fp[count]+fn[count])\n",
        "    if f1[count] > maxx:\n",
        "      max_f1 = count\n",
        "      maxx = f1[count]\n",
        "    count += 1  \n",
        "\n",
        "  tp_f[tasks] = tp\n",
        "  tn_f[tasks] = fp\n",
        "  fp_f[tasks] = fp\n",
        "  fn_f[tasks] = fn\n",
        "  f1_f[tasks] = f1\n",
        "  maxx_f[tasks] = maxx\n",
        "  maxx_f1_f[tasks] = max_f1\n",
        "  map[max_f1] = maxx\n",
        "sorted_map = dict(sorted(map.items(), reverse = True)) #Check order of sorting\n",
        "#Find the subcompoenent with most maxx\n",
        "comp_counter = {}\n",
        "count = 1\n",
        "for tasks in maxx_f1_f.keys():\n",
        "  component = maxx_f1_f[tasks]\n",
        "  try:\n",
        "    temp = comp_counter[component]\n",
        "    temp += 1\n",
        "    comp_counter[component] = temp\n",
        "  except:\n",
        "    comp_counter[component] = 1\n",
        "    count += 1\n",
        "sorted_comp_counter = {k: v for k, v in sorted(comp_counter.items(), reverse=True, key=lambda item: item[1])}\t\t"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Find the results\n"
      ],
      "metadata": {
        "id": "ZFvUE_2ulcZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from scipy import stats\n",
        "\n",
        "print (\"Loading the node-edge ratio\")\t\t\t\n",
        "new_file = open(\"./drive/MyDrive/EDBT23/files/results/ratio.pkl\", \"rb\")\n",
        "ratio = jl.load(new_file)\n",
        "new_file.close()\n",
        "\n",
        "sd_fs = 0\n",
        "sd_tp = 0\n",
        "sd_fn = 0\n",
        "sd_fp = 0\n",
        "mean_fs = 0\n",
        "mean_tp = 0\n",
        "mean_fn = 0\n",
        "mean_fp = 0\n",
        "count = 0\n",
        "tau_f1 = 0\n",
        "tau_tp = 0\n",
        "tau_fp = 0\n",
        "tau_fn = 0\n",
        "tau_f1_c = 0\n",
        "tau_tp_c = 0\n",
        "tau_fp_c = 0\n",
        "tau_fn_c = 0\n",
        "for tasks in f1_f.keys():\n",
        "  f1 = f1_f[tasks]  \n",
        "  tp = tp_f[tasks]\n",
        "  fp = fp_f[tasks]\n",
        "  fn = fn_f[tasks]\n",
        "  rat = {}\n",
        "  all_list = list(f1.keys())\n",
        "  for it1 in ratio.keys():\n",
        "    if it1 in all_list:\n",
        "      rat[it1] = ratio[it1] \n",
        "  sorted_ra = dict(sorted(rat.items(), reverse = True)) #Check order of sorting\n",
        "  rank1 = list(sorted_ra.keys())  \n",
        "  sorted_f1 = dict(sorted(f1.items(), reverse = True)) #Check order of sorting\n",
        "  rank2 = list(sorted_f1.keys())\n",
        "  tau, p_value = stats.kendalltau(np.array(rank1), np.array(rank2))\n",
        "  if p_value < 0.05:\n",
        "    tau_f1 += tau\n",
        "    tau_f1_c += 1\n",
        "  sorted_tp = dict(sorted(tp.items(), reverse = True)) #Check order of sorting\n",
        "  rank2 = list(sorted_tp.keys())\n",
        "  tau, p_value = stats.kendalltau(np.array(rank1), np.array(rank2))\n",
        "  if p_value < 0.05:\n",
        "    tau_tp += tau\n",
        "    tau_tp_c += 1  \n",
        "  sorted_fp = dict(sorted(fp.items(), reverse = True)) #Check order of sorting\n",
        "  rank2 = list(sorted_fp.keys())\n",
        "  tau, p_value = stats.kendalltau(np.array(rank1), np.array(rank2))\n",
        "  if p_value < 0.05:\n",
        "    tau_fp += tau\n",
        "    tau_fp_c += 1  \n",
        "  sorted_fn = dict(sorted(fn.items(), reverse = True)) #Check order of sorting\n",
        "  rank2 = list(sorted_fn.keys())\n",
        "  tau, p_value = stats.kendalltau(np.array(rank1), np.array(rank2))\n",
        "  if p_value < 0.05:\n",
        "    tau_fn += tau\n",
        "    tau_fn_c += 1\n",
        "\n",
        "  it1 = f1[list(sorted_comp_counter.keys())[0]]  \n",
        "  mean_fs += it1\n",
        "\n",
        "  it1 = tp[list(sorted_comp_counter.keys())[0]]  \n",
        "  mean_tp += it1\n",
        "\n",
        "  it1 = fp[list(sorted_comp_counter.keys())[0]]  \n",
        "  mean_fp += it1  \n",
        "\n",
        "  it1 = fn[list(sorted_comp_counter.keys())[0]]  \n",
        "  mean_fn += it1 \n",
        "  count+= 1 \n",
        "mean_fs = mean_fs/count\n",
        "mean_tp = mean_tp/count\n",
        "mean_fp = mean_fp/count\n",
        "mean_fn = mean_fn/count\n",
        "f1_f1 = []\n",
        "tp_f1 = []\n",
        "fp_f1 = []\n",
        "fn_f1 = []\n",
        "for tasks in f1_f.keys():\n",
        "  f1 = f1_f[tasks]  \n",
        "  it1 = f1[list(sorted_comp_counter.keys())[0]]    \n",
        "  sd_fs += (mean_fs-it1)**2\n",
        "  f1_f1.append(it1)\n",
        "  tp = tp_f[tasks]  \n",
        "  it1 = tp[list(sorted_comp_counter.keys())[0]]  \n",
        "  sd_tp += (mean_tp-it1)**2\n",
        "  tp_f1.append(it1)\n",
        "  fp = fp_f[tasks]  \n",
        "  it1 = fp[list(sorted_comp_counter.keys())[0]]  \n",
        "  sd_fp = (mean_fp-it1)**2  \n",
        "  fp_f1.append(it1)\n",
        "  fn = fn_f[tasks]  \n",
        "  it1 = fn[list(sorted_comp_counter.keys())[0]]  \n",
        "  sd_fn = (mean_fn-it1)**2  \n",
        "  fn_f1.append(it1)\n",
        "max_fs = max(f1_f1)\n",
        "max_tp = max(tp_f1)\n",
        "max_fp = max(fp_f1)\n",
        "max_fn = max(fn_f1)\n",
        "min_fs = min(f1_f1)\n",
        "min_tp = min(tp_f1)\n",
        "min_fp = min(fp_f1)\n",
        "min_fn = min(fn_f1)\n",
        "print (\"Number of files in all subgraph: \", len(all_nodes))\n",
        "print (\"Mean F1 score: \", mean_fs, \"    Standard deviation: \", math.sqrt(sd_fs), \"Maximum F1 score: \", max_fs, \"Minimum F1 score: \", min_fs)\n",
        "print (\"Mean TP score: \", mean_tp, \"    Standard deviation: \", math.sqrt(sd_tp), \"Maximum TP score: \", max_tp, \"Minimum TP score: \", min_tp)\n",
        "print (\"Mean FP score: \", mean_fp, \"    Standard deviation: \", math.sqrt(sd_fp), \"Maximum FP score: \", max_fp, \"Minimum FP score: \", min_fp)\n",
        "print (\"Mean FN score: \", mean_fn, \"    Standard deviation: \", math.sqrt(sd_fn), \"Maximum FN score: \", max_fn, \"Minimum FN score: \", min_fn)\n",
        "print (\"Mean Tau correlation between F score and edge-to-node ratio: \", tau_f1/tau_f1_c)\n",
        "print (\"Mean Tau correlation between TP and edge-to-node ratio: \", tau_tp/tau_tp_c)\n",
        "print (\"Mean Tau correlation between FP and edge-to-node ratio: \", tau_fp/tau_fp_c)\n",
        "print (\"Mean Tau correlation between FN and edge-to-node ratio: \", tau_fn/tau_fn_c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1NYZsp7eUc7",
        "outputId": "327ddfc9-8def-47e8-9ec0-00214bb590b9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the node-edge ratio\n",
            "Number of files in all subgraph:  288\n",
            "Mean F1 score:  0.7661756905521191     Standard deviation:  0.09191418092137596 Maximum F1 score:  0.8311688311688312 Minimum F1 score:  0.701182549935407\n",
            "Mean TP score:  0.8951612903225806     Standard deviation:  0.14826432508750192 Maximum TP score:  1.0 Minimum TP score:  0.7903225806451613\n",
            "Mean FP score:  0.4947916666666667     Standard deviation:  0.08854166666666669 Maximum FP score:  0.5833333333333334 Minimum FP score:  0.40625\n",
            "Mean FN score:  0.04513888888888889     Standard deviation:  0.04513888888888889 Maximum FN score:  0.09027777777777778 Minimum FN score:  0.0\n",
            "Mean Tau correlation between F score and edge-to-node ratio:  0.9999999999999999\n",
            "Mean Tau correlation between TP and edge-to-node ratio:  0.9999999999999999\n",
            "Mean Tau correlation between FP and edge-to-node ratio:  0.9999999999999999\n",
            "Mean Tau correlation between FN and edge-to-node ratio:  0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5RsJB-03Fhc",
        "outputId": "d27d20a1-1a7a-4d08-e1f4-88e3508dd084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove './drive/MyDrive/files/Graph/subgraphs/Tuple_*.tup': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm ./drive/MyDrive/EDBT23/files/Graph/subgraphs/Tuple_*.tup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftfxQV1m3OEL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly-7bIXtoUsp"
      },
      "outputs": [],
      "source": [
        "#TODO1\n",
        "#Open files: Weight, Sorted Betweenness\n",
        "#Find thresholds- algorithmic, percentage\n",
        "#From the weight and full graph file, delete the edges whose weight < threshold\n",
        "#Find the disconnected components\n",
        "\n",
        "#TODO2\n",
        "#For every test set task\n",
        "#For every subconnected components\n",
        "#Find and record F1 score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKTxomQnoVNz"
      },
      "source": [
        "TODO: Result of the final experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvbiHtVgoZw9"
      },
      "outputs": [],
      "source": [
        "#TODO1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}